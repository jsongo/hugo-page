<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no"><meta name=description content="近期，HuggingFace 发布的超过 200 页的超长技术博客，系统性地分享训练先进 LLM 的端到端经验。"><title>HuggingFace发布超200页「实战指南」，从决策到落地「手把手」教你训练大模型 ｜ 机器之心</title>
<link rel=canonical href=https://www.jsongo.top/articles/huggingface-200-page-llm-training-guide/><link rel=stylesheet href=/scss/style.min.951ec1dcac7935ce623fd3cadc5aa224d4cac4c20fd5ebd1819deeee8d4c646b.css><script src=https://res.wx.qq.com/open/js/jweixin-1.6.0.js></script><script>wx.ready(function(){wx.updateAppMessageShareData({title:"HuggingFace发布超200页「实战指南」，从决策到落地「手把手」教你训练大模型 ｜ 机器之心",desc:"近期，HuggingFace 发布的超过 200 页的超长技术博客，系统性地分享训练先进 LLM 的端到端经验。",link:"https://www.jsongo.top/articles/huggingface-200-page-llm-training-guide/",imgUrl:"https://cdn.lyb.pub/upic/1763020394_vBc3MT.png",success:function(){}})})</script><meta property='og:title' content="HuggingFace发布超200页「实战指南」，从决策到落地「手把手」教你训练大模型 ｜ 机器之心"><meta property='og:description' content="近期，HuggingFace 发布的超过 200 页的超长技术博客，系统性地分享训练先进 LLM 的端到端经验。"><meta property='og:url' content='https://www.jsongo.top/articles/huggingface-200-page-llm-training-guide/'><meta property='og:site_name' content='Ethan 的思考札记'><meta property='og:type' content='article'><meta property='article:section' content='Articles'><meta property='article:tag' content='clippings'><meta property='article:published_time' content='2025-11-13T15:48:08+08:00'><meta property='article:modified_time' content='2025-11-13T15:48:08+08:00'><meta property='og:image' content='https://cdn.lyb.pub/upic/1763020394_vBc3MT.png'><meta name=twitter:title content="HuggingFace发布超200页「实战指南」，从决策到落地「手把手」教你训练大模型 ｜ 机器之心"><meta name=twitter:description content="近期，HuggingFace 发布的超过 200 页的超长技术博客，系统性地分享训练先进 LLM 的端到端经验。"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://cdn.lyb.pub/upic/1763020394_vBc3MT.png'><link rel="shortcut icon" href=https://cdn.lyb.pub/mc/favicon.jpg><script src=/js/image-lightbox.js defer></script><script src=/js/top-nav.js defer></script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class=floating-avatar><a href=/><img src=/img/avatar_hu11734250231669917296.jpg width=80 height=80 class=avatar-img loading=lazy alt=Avatar></a></div><nav class=top-nav><div class=top-nav-tabs><a href=/ class=top-nav-tab><span>首页</span>
</a><a href=/tech-frontier/ class=top-nav-tab><span>互联网/前沿进展</span>
</a><a href=/clippings/ class=top-nav-tab><span>剪藏</span></a></div></nav><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu7049013329120086875.jpg width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Ethan 的思考札记</a></h1><h2 class=site-description>Keeping Grit!</h2></div></header><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>首页</span></a></li><li><a href=/tech-frontier/><svg class="icon icon-tabler icon-tabler-infinity" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M9.828 9.172a4 4 0 100 5.656A10 10 0 0012 12a10 10 0 012.172-2.828 4 4 0 110 5.656A10 10 0 0112 12 10 10 0 009.828 9.172"/></svg>
<span>互联网/前沿进展</span></a></li><li><a href=/clippings/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>剪藏</span></a></li><li><a href=/page/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/page/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/page/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li><a href=/about target=_blank><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>About Me</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ul><li><a href=#训练罗盘whywhathow>训练罗盘：Why→What→How</a></li><li><a href=#why>Why</a></li><li><a href=#what>What</a></li><li><a href=#每一个大型模型都始于一个小型消融>每一个大型模型都始于一个小型消融</a></li><li><a href=#选择你的基线>选择你的基线</a></li><li><a href=#选择训练框架>选择训练框架</a></li><li><a href=#设计消融实验>设计消融实验</a></li><li><a href=#理解哪些有效评估>理解哪些有效：评估</a></li><li><a href=#模型架构设计>模型架构设计</a></li><li><a href=#数据管理艺术>数据管理艺术</a></li><li><a href=#堪比马拉松的长周期训练>堪比「马拉松」的长周期训练</a></li><li><a href=#超越基础模型2025-年的后训练阶段>超越基础模型——2025 年的后训练阶段</a></li><li><a href=#基础设施被忽视的关键一环>基础设施：被忽视的关键一环</a></li></ul></nav></div></section><section class="widget tagCloud"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg></div><h2 class="widget-title section-title">Tags</h2><div class=tagCloud-tags><a href=/tags/%E4%BA%92%E8%81%94%E7%BD%91/%E5%89%8D%E6%B2%BF%E8%BF%9B%E5%B1%95/ class=font_size_74>互联网/前沿进展
</a><a href=/tags/area/thinking/ class=font_size_5>Area/Thinking
</a><a href=/tags/%E4%BA%92%E8%81%94%E7%BD%91/%E6%8A%98%E8%85%BE/azure/ class=font_size_5>互联网/折腾/Azure
</a><a href=/tags/area/%E6%80%9D%E8%80%83/ class=font_size_4>Area/思考
</a><a href=/tags/inbox/ class=font_size_4>Inbox
</a><a href=/tags/%E8%BE%93%E5%85%A5/%E4%B9%A6/ class=font_size_4>输入/书
</a><a href=/tags/ai/ai%E5%B7%A5%E5%85%B7/claude/ class=font_size_3>AI/AI工具/Claude
</a><a href=/tags/ai/diffusion/ class=font_size_3>AI/Diffusion
</a><a href=/tags/clippings/ class=font_size_3>Clippings
</a><a href=/tags/study/short-video/ class=font_size_3>Study/Short-Video</a></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/articles/huggingface-200-page-llm-training-guide/><img src=https://cdn.lyb.pub/upic/1763020394_vBc3MT.png loading=lazy alt="Featured image of post HuggingFace发布超200页「实战指南」，从决策到落地「手把手」教你训练大模型 ｜ 机器之心"></a></div><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/articles/huggingface-200-page-llm-training-guide/>HuggingFace发布超200页「实战指南」，从决策到落地「手把手」教你训练大模型 ｜ 机器之心</a></h2><h3 class=article-subtitle>近期，HuggingFace 发布的超过 200 页的超长技术博客，系统性地分享训练先进 LLM 的端到端经验。</h3></div><footer class=article-tags><div><svg class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg>
<a href=/tags/clippings>clippings</a></div></footer><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Thu, Nov 13, 2025</time></div></footer></div></header><section class=article-content><p>近期，HuggingFace 发布的超过 200 页的超长技术博客，系统性地分享训练先进 LLM 的端到端经验。<br><img src=https://cdn.lyb.pub/upic/1763020394_vBc3MT.png loading=lazy alt=assets/local_image_plus/db4c3edc3efcaed5601e9f63f8500ce4_MD5.png|747x772><br>博客的重点是 LLM 开发过程中「混乱的现实」。它坦诚地记录了哪些方法有效、哪些会失败，以及如何应对实际工程中遇到的陷阱。内容基于团队的实际项目经验，特别是他们近期使用 384 块 H100 GPU 训练 3B 参数模型 SmolLM3 的过程。<br>博客中提供了深入的技术细节、代码片段和调试技巧，对于有兴趣亲自构建 LLM 的读者来说非常有指导意义。<br>下面是对博客内容的概述，非常推荐感兴趣的读者阅读原文。</p><ul><li>博客地址：https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook <a class=link href=https://www.jiqizhixin.com/articles/javascript%3A; target=_blank rel=noopener>#positional</a> -encodings&ndash;long-context</li></ul><h1 id=训练罗盘whywhathow>训练罗盘：Why→What→How</h1><p><img src=https://cdn.lyb.pub/upic/1763020956_iTehH8.png loading=lazy alt=assets/local_image_plus/0b631e71913c829cf38b68389b1b6cd2_MD5.png><br>这一部分是在投入技术细节（如何训练）之前，提出了一个关键问题：「你是否真的需要训练这个模型」？<br>鉴于（如 Qwen、Gemma、Llama 等）世界级开源模型层出不穷，大多数人可能并不需要从头开始训练自己的模型。<br><img src=https://cdn.lyb.pub/upic/1763020685_dJ6QTt.png loading=lazy alt=assets/local_image_plus/eaa6b68bc50d02f4933cf4b3deef54e9_MD5.png|816x415></p><h1 id=why>Why</h1><p>文章列举了一些不应该训练模型的错误理由，例如：「我们有闲置算力」、「别人都在做」或「AI 是未来」。<br>然后提供了一个流程图，帮助你思考是否真的训练一个自己的模型。<br><img src=https://cdn.lyb.pub/upic/1763020701_RwRYKl.png loading=lazy alt=assets/local_image_plus/d4272669f417b7739db33579c0e87adf_MD5.png|838x923><br>当你发现：现有模型不可用 —> 提示词工程无法解决 —> 微调无法解决，你就可以考虑从头开始训练了。<br>定制化预训练通常适用于三个主要领域：</p><ul><li><strong>研究：</strong> 你有一个明确的科学问题需要回答。例如，测试新的优化器、探索模型能力（如仅用强化学习）或测试新的数据集（如纯合成数据）。</li><li><strong>生产：</strong> 你的业务有无法被满足的特定需求。如 DNA、法律、金融等高度专业化的词汇或逻辑； 需要在特定硬件（如无人机、本地 FPGA）上运行，或有严格的延迟要求；处于受监管行业，需要对训练数据和模型行为有 100% 的控制和可追溯性。</li><li><strong>战略开源：</strong> 你发现并有能力填补当前开源生态系统中的一个特定空白。</li></ul><h1 id=what>What</h1><p>一旦你明确了「Why」，就可以推导出「训练什么 (What)」。包括模型类型（密集型、MoE、混合型、某种新型）、模型大小、架构细节和数据混合。<br>同时前面的领域目标决定了你的训练决策：例如，为设备端运行 —> 训练小型高效模型；需要多语言能力 —> 使用更大的 tokenizer 词汇表；超长上下文 —> 混合架构。<br>这个决策过程分为两个阶段。 <strong>规划</strong> ：将你的约束（来自「Why」）映射到具体的模型规格； <strong>验证</strong> ：通过系统性的实验（消融实验）来测试你的选择。<br>文章指出了成功 LLM 训练团队的两个关键特质：</p><ul><li><strong>迭代速度：</strong> 训练 LLM 是一个「边训练边学」的过程。能够快速、频繁地（例如每季度而不是每年）迭代训练新模型的团队，会进步得更快。</li><li><strong>数据管理：</strong> 最优秀的团队是那些「痴迷于高质量数据」的团队，数据质量的影响远超架构选择。<br>文章还建议，预训练团队一开始不需要很多人（2-3 人足矣），关键是配备足够的算力并保持快速迭代。</li></ul><h1 id=每一个大型模型都始于一个小型消融>每一个大型模型都始于一个小型消融</h1><p>在开始训练 LLM 之前，需要做出一系列关键决策（架构、优化器、数据组合等）。人们常以为这些决策是靠深思熟虑得出的，但仅凭推理是不够的，因为 LLM 的行为常常反直觉。<br>一个典型的例子是：使用看似「最高质量」的 arXiv 科学论文数据，反而可能会损害模型（尤其是小模型）的性能，因为它过于专业化，缺乏通用文本的多样性。<br>既然纯粹的思考行不通，答案就是像经验主义者一样「运行大量实验」（即消融实验）。<br>设置消融实验的完整流程：</p><h1 id=选择你的基线>选择你的基线</h1><p>不要从零开始，应该选择一个已被验证的、成熟的架构（如 Llama 3.1、Qwen3、Gemma3）作为起点，这样可以继承所有已知的优化和稳定性经验。<br><img src=https://cdn.lyb.pub/upic/1763020463_RGNFVE.png loading=lazy alt=assets/local_image_plus/791c6ba0b0ab5483ea74fa1f16272368_MD5.png|838x511><br>基线虽好，但并非为你量身定制，因此需要修改。然而，「任何架构上的改变都伴随着风险」。为此，必须遵守「去风险」的纪律，即：「除非你测试过它确实有帮助，否则不要改变任何东西。」<br>修改的难点在于组件太多且相互作用。你不能测试所有组合。正确的方法是： <strong>一次只测试一个有潜力的变更</strong> 。如果它有效，就将其整合，使其成为新的基线，然后再测试下一个变更。</p><h1 id=选择训练框架>选择训练框架</h1><p>这是一个关键的技术决策，需要在功能、稳定性和吞吐量之间权衡。<br>文章对比了几个主流框架：</p><ul><li>Megatron-LM / DeepSpeed：功能强大，经过实战考验，但代码库庞大且复杂。</li><li>TorchTitan：更轻量级，易于上手和实验，但相对较新。</li><li>nanotron (作者自研)：提供了完全的灵活性，但需要大量投入来开发和测试。<br><img src=https://cdn.lyb.pub/upic/1763020469_UHfZEN.png loading=lazy alt=assets/local_image_plus/f819c5f2cabbcb50d738552dbbe415f7_MD5.png|838x330></li></ul><h1 id=设计消融实验>设计消融实验</h1><p>实验必须足够快（以便快速迭代）和足够可靠（结果能外推到最终模型），有两种主要方法：</p><ul><li>全尺寸模型，少量数据： 使用最终模型的尺寸（如 SmolLM3 使用 3B 模型），但在更少的 Token 上训练（如 100B 而非 11T）。</li><li>小型代理模型： 如果目标模型太大（如 1T 参数），则使用一个按比例缩小的代理模型（如 3B 模型）进行实验。<br>接下来文章介绍了其基准消融设置（1B 的 Llama 模型，训练 45B Token），并展示了配置文件的关键部分（数据、模型、优化器等）。</li></ul><h1 id=理解哪些有效评估>理解哪些有效：评估</h1><p>文章指出，评估实验结果时，只看训练损失 (Loss) 是不可靠的。例如，训练维基百科的 Loss 更低，但不代表模型能力更强；更换分词器也会导致 Loss 无法直接比较。因此，必须使用更细粒度的下游评估。<br>一个可靠的评估任务应具备四个标准：单调性、低噪声、超随机性能和排名一致性。<br>特别是在早期实验中，「完形填空（CF）」格式比「多项选择（MCF）」更优越，因为后者（如 MMLU）在模型训练的早期阶段表现接近随机，无法提供有效的早期信号。<br>消融实验的真正价值不仅在于构建好模型，更在于它为未来的调试提供了信心：当主训练不可避免地出错时，系统性的实验结果能帮助团队快速定位问题。<br>不过，这种价值的成本极其昂贵。以 SmolLM3 为例，消融和调试所消耗的 GPU 时间超过了主训练运行的一半。<br><img src=https://cdn.lyb.pub/upic/1763020702_mNYIkk.png loading=lazy alt=assets/local_image_plus/9501460f026c2f40ea25f3a0105e0b2e_MD5.png|838x27></p><h1 id=模型架构设计>模型架构设计</h1><p>这部分内容详细阐述了设计和确定 LLM 架构的完整决策过程，从高层目标到具体的组件选择和超参数设置。<br>文章以一个名为 SmolLM3 的 3B（30 亿参数）模型为例，系统性地展示了如何从零开始构建一个模型的「蓝图」。<br>文章深入探讨了构成现代 Transformer 的核心架构选择并指出，当今的模型（如 Qwen3、Gemma3）共享 Transformer 基础，但通过组件改进（如 GQA、位置编码）来解决具体问题（如内存、稳定性）。</p><ul><li><strong>注意力机制：</strong> 这是推理时的主要瓶颈，关键在于 KV 缓存。文章对比了 MHA（标准，高内存）、MQA（极端压缩，可能损失性能）和 GQA（分组查询）。消融实验证实，GQA 在性能上与 MHA 相当，但极大节省了 KV 缓存，是 SmolLM3 的最终选择。</li><li><strong>长上下文：</strong> 文章探讨了两种策略。首先是 <strong>文档掩码</strong> ，在训练「打包」的数据时，它能防止模型关注到序列中不相关的其他文档，这被证实对长上下文扩展至关重要。其次是 <strong>位置编码</strong> ，标准 RoPE 在长序列上外推能力有限。SmolLM3 采用了 NoPE（实为 RNoPE）的混合策略，即交替使用 RoPE 层（处理短上下文）和 NoPE 层（处理长距离检索），消融实验表明这种方法在不牺牲短上下文性能的同时，为长上下文打下了基础。</li><li><strong>嵌入共享：</strong> 对于 SmolLM3 这样的小模型，嵌入层占比较大。文章通过消融实验证明，将参数用于增加模型深度（更多层）比用于「解绑」输入和输出嵌入层更有效。因此，SmolLM3 采用了嵌入共享。</li><li><strong>稳定性：</strong> 为防止大规模训练崩溃，文章测试了 Z-loss、QK-norm 等技术。最终，SmolLM3 采用了 OLMo2 的技巧，即移除嵌入层的权重衰减，以提高稳定性。<br>文章对比了密集型、MoE（混合专家）和 Hybrid（混合模型）三种架构。MoE 通过稀疏激活（只激活部分「专家」）来用更少的计算换取更大的容量，但内存占用极高。Hybrid（如 Mamba）则通过线性注意力或 SSM 来解决 Transformer 在长上下文上的计算瓶颈。SmolLM3 因其「端侧部署」的目标（内存受限）而坚持使用密集型架构。<br>随后，文章转向了常被低估的 <strong>Tokenizer</strong> 。选择分词器涉及词汇量大小（影响压缩率和嵌入矩阵大小）和算法（BPE 最常用）。<br>文章引入了「Fertility」（每词平均 Token 数）和「连续词比例」作为评估指标。通过对比 Llama3、Gemma3、Qwen3 等，SmolLM3 最终选择了 Llama3 的 128k 词汇表，因为它在目标语言和模型大小之间取得了最佳平衡。<br>接下来，文章探讨了决定训练过程的核心要素：优化器、学习率和批量大小。文章指出，直接借用其他模型的超参数虽然简单，但可能不是最优的，因为这些值是针对特定的架构、数据和约束条件优化的。<br>最后回顾了关于模型规模（参数量 N）和数据量（Token 数 D）的经典权衡。</li></ul><h1 id=数据管理艺术>数据管理艺术</h1><p>这部分内容详细阐述了「数据策展的艺术」，强调了在 LLM 训练中，数据是决定模型「学到什么」的关键因素，其重要性甚至超过了模型架构。<br>模型架构决定了模型如何学习，而数据则决定了模型学习的内容。如果数据质量差或「混合比例」不当，再好的架构或超参数也无法挽救。<br>文章指出，构建一个优秀的数据集并不仅仅是收集好数据，而是要设计一个 <strong>训练混合</strong> 。<br>例如，过分增加代码数据的比例（「上采样」）会隐式地减少其他数据的比例，可能损害模型的通用能力。<br>此外，对于像 SmolLM3 这样需要 11T Token 的超长训练，如果只使用「最高质量」的数据，将导致严重的数据重复，这对模型性能有害。<br>为了解决这些平衡性问题，现代 LLM 训练已经从「静态混合」（如 GPT-3）演变为多阶段训练（如 Llama3、SmolLM2）。这种方法在训练过程中动态地改变数据混合比例。<br>其核心洞察是， <strong>模型的最终行为深受其在训练末期看到的数据的影响</strong> 。因此，策略是：</p><ul><li>在训练早期，使用丰富、多样化但质量稍低的数据（如网页文本）。</li><li>在训练末期（特别是在学习率衰减的「退火阶段」），引入稀缺、高质量的数据（如专业数学和代码数据集），以最大化其影响力。<br>何时改变混合比例通常由性能驱动的干预决定：例如，当发现模型的数学能力停滞不前时，就是引入更多高质量数学数据的信号。<br>确定数据配方的过程依赖于系统的消融实验。与架构不同，数据混合的消融实验必须在目标模型规模（例如 3B）上运行，因为模型的容量会显著影响它吸收不同数据的效果。<br>文章介绍了两种主要的实验方法：</li><li><strong>从零开始的消融</strong> ：使用目标模型（如 3B）进行短期训练（如 100B Token），以测试不同的初始混合比例。</li><li><strong>退火实验</strong> ：这是测试多阶段课程的关键。团队会从主训练中（例如在 7T Token 处）获取一个检查点，然后用新的数据混合（例如 40% 基线 + 60% 新数学数据）继续训练一小段时间（如 50B Token），以验证新数据在后期引入的有效性。<br>作者提到，尽管存在 DoReMi 等自动优化方法，但在他们的实践中，仔细的手动消融实验仍然是 SOTA 模型（包括 SmolLM3）确定数据混合的最佳途径。<br>文章最后以 SmolLM3 为例，展示了如何应用这些原则。</li></ul><h1 id=堪比马拉松的长周期训练>堪比「马拉松」的长周期训练</h1><p>从前面来看，此时已经准备好了大部分的工作，经过验证的模型架构、最终确定的数据混合方案、调好的超参数，剩下的任务就是搭建好基础设施（这在最后讲解），然后「开始」训练。而训练是一个堪比「马拉松」的长周期过程，过程中可能会出现各种情况，所以要做好面对各种挑战的准备。<br>而这部分主要讲的就是，训练前的「飞行前检查」、过程中那些不可避免的意外状况，以及如何保持系统稳定、不中断。<br>文章以启动 SmolLM3 前执行的「起飞前检查」清单为例，展示了在开始训练前的准备工作，包括基础设施准备、评测系统准备、Checkpoint 与自动恢复机制、指标日志记录、训练配置复核等。<br>尤其是在最后按下「训练」按钮之前的训练配置复核，一定要仔细检查训练配置文件、启动脚本、Slurm 提交命令等，以确保参数、路径、环境变量都正确无误。<br>当然，即使做好了万全准备，在规模化训练过程中，也依然会遇到一些问题。比如在训练启动后的短短数小时内系统的吞吐率（throughput）骤然下滑、持续下滑，以及在引入新的 dataloader（数据加载器） 后，虽然吞吐率下降的问题不再出现，但损失曲线（loss curve）却明显变得更加噪声化，波动比以前大得多等等，各种问题随时都会出现，所以要做好及时应对各种问题的准备。<br>另外，文章还指出，在现代 LLM 的预训练中，通常会采用多阶段训练策略（multi-stage training），每个阶段使用不同的数据混合比例，并在最后阶段进行上下文长度扩展。比如 Qwen3 就采用了通用阶段、推理阶段、长上下文阶段的三阶段训练方案。而 SmolLM3 采用了类似的理念，在训练过程中计划性地引入高质量数据集并扩展上下文长度，同时根据性能监控结果进行动态调整。</p><h1 id=超越基础模型2025-年的后训练阶段>超越基础模型——2025 年的后训练阶段</h1><p>这部分主要介绍了模型的后训练（Post-training）。以 SmolLM3 为例，在完成预训练（Pre-training）后就拥有了 SmolLM3 的原始能力（raw ability），但在 GPU 的温度还未降下之前，就进入了后训练（Post-training）阶段。<br><img src=https://cdn.lyb.pub/upic/1763020708_tTL9CJ.png loading=lazy alt=assets/local_image_plus/21ef50bee6411684fb48d0b1e0ebfac0_MD5.png><br>当然，在这一切开始之前，就像预训练阶段一样，你也要问自己三个问题：</p><ul><li><strong>你是不是真的需要后训练？</strong> 如今许多开源权重模型在各种任务上已能媲美闭源模型，其中一些甚至可以在本地运行（通过量化与低计算配置）。如果你的目标只是一个通用助手，那么 Hugging Face Hub 上的现成模型可能已经足够好，没必要重新训练。</li><li><strong>你是否拥有高质量、领域特定的数据？</strong> 后训练的最大价值体现在特定任务或领域上。若通用模型在这些场景下表现欠佳，高质量的专用数据能让你定向优化输出效果。</li><li><strong>你能衡量成功的标准吗？</strong> 如果没有清晰的评估标准，你将无法判断后训练是否真的给你带来了改进。<br>如果确定了要进行后训练，那么又出现一个问题，你想要后训练实现什么目标：一个严格执行指令、几乎不偏题的模型？一个多才多艺的助手，能灵活切换语气与角色？一个擅长数学、代码或推理任务的「思考引擎」？还是一个能多语言流畅交流的通用对话体？<br>只有明确目标才能选择合适的技术路线。<br>而一旦前面这几个问题答案都明确之后，接下来就要开始进行训练了，主要步骤包括：</li><li><strong>监督微调（SFT）</strong> ：注入核心任务能力；</li><li><strong>偏好优化（PO）</strong> ：直接从人类或 AI 偏好中学习；</li><li><strong>强化学习（RL）</strong> ：在监督数据之外提升模型的可靠性与推理深度；</li><li><strong>数据筛选与整理（Data Curation）</strong> ：平衡数据的多样性与质量；</li><li><strong>评估体系（Evaluation）</strong> ：持续跟踪进展并及早发现性能回退。<br>文章以 SmolLM3 为例，回答了在进行后训练阶段需要回答的几大问题：<br>SmolLM3 是一个优秀的基础模型，但要在发布前变得可用，必须经过后训练。同时，混合推理模型（如 Qwen3 系列）正快速兴起，但开源社区中缺乏公开可复现的训练配方。因此，SmolLM3 的后训练目标有两点：打造一个可实用的高质量模型；贡献一份完整开源的训练方案，让它能与 Qwen3 的 1.7B 和 4B 模型一同位列行业前沿。<br>而在后训练的实战阶段时，需要做很多事情，比如选择后训练框架、工具等。不同的框架各自支持不同的算法类型、微调方法、可扩展能力等。<br>文章总结了一些主要的框架在后训练各环节中的支持范围，涵盖从监督微调到偏好优化，再到强化学习等核心领域的能力对比。<br><img src=https://cdn.lyb.pub/upic/1763020542_fnYMAB.png loading=lazy alt=assets/local_image_plus/65c1e55b46bd58e48bac89e76a70f2ad_MD5.png|838x586><br>而在主要步骤阶段，文章解答了为何几乎所有的后训练流程都是以监督微调为起点，原因很简单：</li><li><strong>便宜</strong> ：相较于 RL，SFT 对算力要求低得多。你通常可以在较短时间内、用较少 GPU，获得显著性能提升——而无需「烧光硅片」。</li><li><strong>稳定</strong> ：不同于 RL 那种对奖励设计和超参数极度敏感的训练方式，SFT「开箱即用」——几乎不会崩。</li><li><strong>是最好的基线</strong> ：一个良好的 SFT 检查点（checkpoint）通常能提供你所需的大部分性能提升，并让后续如 DPO 或 RLHF 等方法的训练更加高效。</li></ul><h1 id=基础设施被忽视的关键一环>基础设施：被忽视的关键一环</h1><p>这部分主要是将基础设施，因为大多数从事模型训练的人都非常关心模型架构和数据质量，而忽视了底层的基础设施，认为「租几块 GPU，撞上 Pytorch 就可以了」。然而并非如此，如果用一个比喻来形容，那就是 <strong>「预训练是蛋糕坯，后训练是上面的糖霜和樱桃，而基础设施就是工业级烤箱」</strong> 。没有它，一切无从谈起。<br>像在 <strong>训练 SmolLM3 时，使用了 384 块 H100 GPU，持续了将近一个月，总共处理了 11 万亿个 token</strong> ，工程量之浩大，过程之繁琐。<br>文章指出，对于基础设施，你首先需要知道的是，GPU 的构成、内存层级的工作方式、CPU 与 GPU 之间的通信方式、获取 GPU 时的注意事项，以及在投入长期训练任务前如何测试它们。<br><img src=https://cdn.lyb.pub/upic/1763020553_YTac5k.png loading=lazy alt=assets/local_image_plus/42eb61c8281f9295d806c50932a2de90_MD5.png|838x586></p><p><strong>CPU 与 GPU 之间的通信路径</strong><br>其中，需要注意的是，在大型模型训练中，拥有足够多且高速的 GPU 固然重要，但由于 LLM 训练通常持续数周甚至数月，持续追踪 GPU 的健康状态就成为了保持训练稳定性的关键。<br>文章以 SmolLM3 的训练为例，列举了对 GPU 进行全面诊断的工具：</p><ul><li><strong>GPU Fryer</strong> （内部工具）：一款 GPU 压力测试工具，用于检测是否存在热降频；显存错误；性能异常等潜在问题。</li><li><strong>NVIDIA DCGM</strong> （数据中心 GPU 管理器）：一款被广泛使用的 GPU 诊断与监控工具，能够执行深度检测，以验证 GPU 硬件、监控性能，并定位故障或功率异常的根本原因。诊断范围包括：计算单元完整性；PCIe 连接稳定性；内存完整性；热稳定性等。<br>最后，关于训练模型到底要用多少块 GPU，文章指出决策的核心在于训练时间、成本与扩展效率之间权衡的过程。用一个公式来估算就是：<br><img src=https://cdn.lyb.pub/upic/1763020560_lh3w8F.png loading=lazy alt=assets/local_image_plus/f7a458795d9a2d9cfaaf68727b8b0533_MD5.png><br>其中，所需总 FLOPs，训练模型所需的计算量，取决于模型规模、训练 token 数量和架构设计；单 GPU 吞吐量，即每张 GPU 际每秒可执行的 FLOPs 数量；目标训练时长，就是你期望训练完成所需的时间。<br>以 SmolLM3 为例，根据模型规模 30 亿参数、训练 token 数：11 万亿、目标训练时间约 4 周等信息，代入 GPU 需求公式得出的结果约为 379 GPUs。<br>这一计算结果指向了一个合理的范围：约 375–400 张 H100 GPU，而最后实际上是部署了 384 张 H100，这一规模既符合我们的并行化策略（parallelism strategy），也为训练中可能出现的节点故障、重启等意外情况预留了充足的缓冲空间，从而确保模型能在约 4 周时间内顺利完成训练。<br>而这也再次证明基础设施对于模型训练的重要性，不要忽视它！</li></ul></section><footer class=article-footer><div class=article-time-tags-wrapper><section class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Thu, Nov 13, 2025</time></div></section><section class=article-tags><a href=/tags/clippings/>Clippings</a></section></div></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/articles/langchain-manus-context-engineering-best-practices/><div class=article-image><img src=https://cdn.lyb.pub/upic/1763024638_8fSpoH.webp loading=lazy data-key=langchain-manus-context-engineering-best-practices data-hash=https://cdn.lyb.pub/upic/1763024638_8fSpoH.webp></div><div class=article-details><h2 class=article-title>LangChain联合Manus季逸超最新分享！也许当前最好的「上下文工程」讲解</h2></div></a></article></div></div></aside><script src=https://giscus.app/client.js data-repo=jsongo/hugo-page data-repo-id=R_kgDONQf4yA data-category=Announcements data-category-id=DIC_kwDONQf4yM4CkfAr data-mapping=title data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><script>function setGiscusTheme(e){let t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:{setConfig:{theme:e}}},"https://giscus.app")}(function(){addEventListener("message",t=>{if(event.origin!=="https://giscus.app")return;e()}),window.addEventListener("onColorSchemeChange",e);function e(){setGiscusTheme(document.documentElement.dataset.scheme==="light"?"preferred_color_scheme":"dark_dimmed")}})()</script><footer class=site-footer><section class=copyright>&copy;
2024 -
2026 Ethan 的思考札记</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.29.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>