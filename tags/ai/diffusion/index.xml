<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI/Diffusion on Ethan 的思考札记</title><link>https://www.jsongo.top/tags/ai/diffusion/</link><description>Recent content in AI/Diffusion on Ethan 的思考札记</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sun, 01 Dec 2024 11:29:47 +0800</lastBuildDate><atom:link href="https://www.jsongo.top/tags/ai/diffusion/index.xml" rel="self" type="application/rss+xml"/><item><title>图像生成：角色场景形象适配</title><link>https://www.jsongo.top/articles/ipadapter-lora-controlnet/</link><pubDate>Sun, 01 Dec 2024 11:29:47 +0800</pubDate><guid>https://www.jsongo.top/articles/ipadapter-lora-controlnet/</guid><description>&lt;img src="https://cdn.ethango.top/banners/3c463375cc35ddc5e78d71b95ddcbd51.jpeg" alt="Featured image of post 图像生成：角色场景形象适配" />&lt;hr>
&lt;h1 id="几种方案">几种方案
&lt;/h1>&lt;h2 id="ip-adapter">IP-Adapter
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>原理&lt;/strong>：IP-Adapter 的核心是其 “解耦交叉注意力机制”，它将文本特征和图像特征的处理分开，为每种输入类型创建了不同的路径，从而能够更好地将视觉提示信息整合到模型中，同时不影响模型对文本指令的理解和执行能力。通过这种机制，IP-Adapter 可以识别参考图的艺术风格、内容以及特定的角色形象等信息，并生成与之相似的作品。&lt;/li>
&lt;li>&lt;strong>使用方式&lt;/strong>：以 Stable Diffusion 为例，需先下载对应版本的 IP-Adapter 预处理器和模型，如 sd1.5 版本的 “ip-adapter_clip_sd15” 预处理器及相关模型。在使用时，将打斗场景图作为输入图像，将角色形象图作为参考图，通过调整相关参数和权重，IP-Adapter 就能根据角色形象图的特征对打斗场景图中的角色进行适配和修改，生成符合预期的新图像。&lt;/li>
&lt;/ul>
&lt;h2 id="dreambooth">DreamBooth
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>原理&lt;/strong>：DreamBooth 是一种基于扩散模型的微调技术，它允许用户使用少量的特定主题图像（如你的角色形象图）对预训练的文本到图像生成模型进行微调，使模型能够学习到与该主题相关的特定特征和细节，从而在生成图像时能够更准确地生成符合该主题的内容。&lt;/li>
&lt;li>&lt;strong>使用方式&lt;/strong>：首先收集包含角色形象的高质量图像数据集，然后选择一个预训练的基础模型，如 Stable Diffusion 等。使用 DreamBooth 技术对基础模型进行微调，将角色形象与特定的唯一标识符绑定，让模型学习到角色的特征。最后，将微调后的模型应用于打斗场景图，通过输入相关的文本提示，模型会根据学习到的角色特征对场景图中的角色进行适配和生成。&lt;/li>
&lt;/ul>
&lt;h2 id="lora-low-rank-adaptation-of-large-language-models">LoRA (Low-Rank Adaptation of Large Language Models)
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>原理&lt;/strong>：LoRA 的基本思想是通过在大型语言模型的基础上，添加少量的可训练参数来实现对模型的微调，从而在保持模型原有性能的基础上，快速适应特定的任务或领域。对于图像生成任务，同样可以利用类似的原理，通过对预训练的图像生成模型添加 LoRA 模块，使其能够根据特定的角色形象图进行微调，学习到角色的特征表示，进而在生成图像时能够更好地适配这些角色形象。&lt;/li>
&lt;li>&lt;strong>使用方式&lt;/strong>：先确定预训练的图像生成模型，然后针对你的角色形象图准备相应的数据集。接着，设计并训练适合该模型的 LoRA 模块，使其能够捕捉到角色形象的关键特征。在实际应用中，将训练好的 LoRA 模块与预训练模型结合，输入包含打斗场景和角色相关的文本提示，模型就会根据 LoRA 模块中学习到的角色特征，对场景图中的角色进行相应的适配和生成。&lt;/li>
&lt;/ul>
&lt;h2 id="controlnet">ControlNet
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>原理&lt;/strong>：ControlNet 是一种用于控制图像生成过程的神经网络架构，它可以通过添加额外的条件控制信息来引导图像生成模型生成符合特定要求的图像。在你的需求中，可以将角色形象图作为 ControlNet 的输入条件，通过网络中的特定模块对生成过程进行约束和引导，使生成的打斗场景图中的角色能够更好地适配给定的角色形象.&lt;/li>
&lt;li>&lt;strong>使用方式&lt;/strong>：选择合适的 ControlNet 预训练模型，并将其与基础的图像生成模型结合，如 Stable Diffusion。将打斗场景图作为基础图像输入，同时将角色形象图作为 ControlNet 的条件输入，通过调整 ControlNet 中的各种参数和权重，如控制线条、色彩、形状等方面的参数，来精确控制生成的图像中角色的外观和姿态，使其与给定的角色形象相匹配。&lt;/li>
&lt;/ul>
&lt;h1 id="成本分析">成本分析
&lt;/h1>&lt;ul>
&lt;li>&lt;strong>IP-Adapter&lt;/strong>：IP-Adapter 无需进行模型训练，主要成本在于寻找符合预期的参考图，其成本相对较低，能在几分钟内看到结果，大大节省了时间。&lt;/li>
&lt;li>&lt;strong>DreamBooth&lt;/strong>：训练成本相对较高，需要至少 20 张不同角度的人物图像，并且对硬件要求较高，至少需要 24GB 的显存以及性能较好的 GPU，若使用云服务训练则还需额外产生费用。&lt;/li>
&lt;li>&lt;strong>LoRA&lt;/strong>：LoRA 可以冻结预训练模型的全部参数权重，然后只在新增的网络层去做训练，训练数据量可根据具体需求而定，因此成本相对较低。&lt;/li>
&lt;li>&lt;strong>ControlNet&lt;/strong>：ControlNet 的训练成本相对较高。例如，其 canny edge detector 模型的训练需要 300 万张边缘 - 图像 - 标注对的语料，在 A100 80g 的环境下要花费 600 个 gpu 小时；human pose 模型则需 8 万张姿态 - 图像 - 标注对的语料及 400 个 gpu 时。&lt;/li>
&lt;/ul>
&lt;h1 id="思路">思路
&lt;/h1>&lt;h2 id="lora">LoRA
&lt;/h2>&lt;h3 id="过拟合问题">过拟合问题
&lt;/h3>&lt;p>需要准备多张形象比较一致的图片，要不然只有一张图会有&lt;strong>过拟合问题&lt;/strong>，以及&lt;strong>特征不全面&lt;/strong>的问题。简单说，仅靠一张图片，LoRA 可能无法学习到角色的完整特征，导致在生成场景图中角色的其他角度或状态时出现不匹配的情况。例如，角色在训练图片中是正面微笑的形象，在生成侧面打斗场景时，可能会出现面部结构变形或者表情不自然的问题。&lt;br>
这个方法可以作为后续优化的方向，当可以生成多张形象一致的图片时，再来试试这个思路。&lt;/p>
&lt;h3 id="猫箱类软件">猫箱类软件
&lt;/h3>&lt;p>这些软件会让使用者先上传几张自己的图片，之后它其实会拿这些去做 LoRA 训练（也可能是其它方式的微调， 如 DreamBooth），之后再用这个用户专属的模型去生成他后续的各场景图。这类软件使用方式上，稍微有些麻烦，因为上传图片用户得好好挑挑，都上传同一时间或穿着、光线下的图片时，其实效果也一般也容易过拟合。不过训练完后，就可以非常自由的生成非常丰富、多样的场景下的个人图片，弄得好的话非常逼真，真假难辨。&lt;/p>
&lt;h2 id="controlnet-1">ControlNet
&lt;/h2>&lt;h3 id="硬件要求">硬件要求
&lt;/h3>&lt;p>ControlNet 需要一定的硬件支持来运行，由于它在原始扩散模型基础上增加了控制模块，对 GPU 等硬件资源的需求相对较高，特别是在处理复杂的控制条件和高分辨率图像生成时，可能需要更强大的 GPU 来保证生成速度和质量。&lt;/p>
&lt;h3 id="时间成本">时间成本
&lt;/h3>&lt;p>训练 ControlNet 所需的时间较长，从数据收集、标注到模型训练完成，整个过程可能需要数天甚至数周，这取决于训练数据的规模和硬件设备的性能。&lt;/p>
&lt;blockquote>
&lt;p>总体看，成本上不一定划得来。相比之下，IP-Adapter 不需要训练模型，只需找到符合预期的参考图即可开始生成，大大节省了时间和资源，能够在几分钟内看到结果，其模型参数只有 22m，存储占用小，对硬件资源的要求相对较低，下面展开讲下&lt;/p>
&lt;/blockquote>
&lt;h2 id="ip-adapter-1">IP-Adapter
&lt;/h2>&lt;p>IP-Adapter 确实有其独特的优势，使其成为一种很有吸引力的图像生成辅助方法，以下是对其优点的详细介绍。&lt;/p>
&lt;h3 id="无需训练模型">无需训练模型
&lt;/h3>&lt;p>与需要收集大量数据并进行长时间训练的 LoRA 等方法不同，IP-Adapter 不需要训练模型，大大节省了时间和资源，仅需找到符合预期的参考图即可开始生成，能在几分钟内看到结果，效率极高。&lt;/p>
&lt;h3 id="单图即可实现风格迁移">单图即可实现风格迁移
&lt;/h3>&lt;p>只需一张图片就能实现风格迁移，将该图片的风格特征融入到生成的图像中。例如，如果有一张具有独特画风的角色图片，IP-Adapter 可以把这种画风应用到新生成的打斗场景图中，让场景图中的角色呈现出与参考图一致的风格，为生成特定风格的图像提供了极大的便利。&lt;/p>
&lt;h3 id="支持多图多特征提取">支持多图多特征提取
&lt;/h3>&lt;p>可以同时读取多张参考图，并提取它们的特征，从而使生成的结果拥有更丰富的多样性和随机性。比如，你有几张同一角色在不同状态下的图片，IP-Adapter 能够综合这些图片的特征，生成更全面、更具变化的角色形象，使其在打斗场景中的表现更加自然和丰富。&lt;/p>
&lt;h3 id="与-prompt-配合度高">与 Prompt 配合度高
&lt;/h3>&lt;p>对 Prompt 有强注意力，能使 Prompt 中的信息更直观地反映在生成结果中。用户可以通过替换 Prompt 里的关键词，在继承参考图风格的同时，指向不同的结果，形成 Prompt 的组合矩阵，进一步拓展生成结果的多样性。例如，在生成打斗场景图时，通过修改 Prompt 中的关键词，如 “激烈打斗”“轻松打斗” 等，可以生成不同氛围和情节的场景图，且都带有参考图的风格特征。&lt;/p>
&lt;h3 id="兼容性强">兼容性强
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>适配多种基础模型&lt;/strong>：IP-Adapter 基于预训练的文本到图像扩散模型开发，具有很强的通用性和兼容性，可以适配 Stable Diffusion 等多种基础模型，而无需针对不同的基础模型进行重新训练或调整.&lt;/li>
&lt;li>&lt;strong>可与 ControlNet 等结合&lt;/strong>：能够与 ControlNet 等其他可控生成工具结合使用，实现更强大的图像生成控制能力。比如，通过 ControlNet 可以对生成图像的线条、边缘等细节进行控制，再结合 IP-Adapter 的风格迁移和内容引导，能够生成更加精细、准确且符合风格要求的图像。&lt;/li>
&lt;/ul>
&lt;h3 id="模型参数小">模型参数小
&lt;/h3>&lt;p>IP-Adapter 本身的模型参数只有 22M，存储占用小，对硬件资源的要求相对较低，在普通的硬件设备上也能够较为流畅地运行，降低了使用门槛，更便于广泛应用和推广。&lt;/p>
&lt;h1 id="方案选择">方案选择
&lt;/h1>&lt;p>最终还是决定先用 IP-Adapter 来推进。&lt;/p></description></item><item><title>Flux 图像生成模型</title><link>https://www.jsongo.top/articles/flux-101/</link><pubDate>Sun, 24 Nov 2024 23:31:34 +0800</pubDate><guid>https://www.jsongo.top/articles/flux-101/</guid><description>&lt;img src="https://cdn.ethango.top/banners/2679aa8e01ac3a9721f4b6f4e919a651.jpg" alt="Featured image of post Flux 图像生成模型" />&lt;h1 id="flux-基础">Flux 基础
&lt;/h1>&lt;blockquote>
&lt;p>Flux 是多模态和并行扩散 Transformer 块的混合架构，拥有 120 亿参数，是目前最大的开源文本到图像模型之一，能够生成高质量、细节丰富且风格多样的图像。&lt;br>
性能和效果可以与 Midjourney V6 媲美。&lt;br>
Flux 一般通过 ComfyUI 来调用。&lt;/p>
&lt;/blockquote>
&lt;h2 id="黑森林实验室与团队背景">黑森林实验室与团队背景
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>核心成员&lt;/strong>：黑森林实验室由 Stable Diffusion 的核心开发者 Robin Rombach 领衔创立，团队成员大多来自 Stability AI，如 Andreas Blattmann、Axel Sauer 等多位原 Stable Diffusion 项目的关键人物，他们在生成式模型开发领域经验丰富，为 Flux 的诞生奠定了坚实基础。大家可能听说过 Stability AI 团队内部不和，各种破事把公司折腾得够呛，所以由原班核心人马出来创业做的 Flux 受到很大的关注。&lt;/li>
&lt;li>&lt;strong>融资情况&lt;/strong>：这个实验室已获得 3200 万美元的种子轮融资，由著名风投机构 Andreessen Horowitz (a16z) 领投，多位业内知名人士参与投资，充足的资金支持使其能够大力推进模型的研发与优化等工作。主要还是因为它的背景实在太好了。&lt;/li>
&lt;/ul>
&lt;h2 id="flux-模型特点">Flux 模型特点
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>架构与参数&lt;/strong>：Flux 基于多模态和并行扩散 Transformer 块的混合架构，拥有 &lt;!-- raw HTML omitted -->120 亿参数&lt;!-- raw HTML omitted -->，是目前最大的开源文本到图像模型之一。这种架构和庞大的参数规模使其能够学习到更丰富的图像特征和语义信息，从而生成高质量的图像。&lt;/li>
&lt;li>&lt;strong>性能优势&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>图像质量&lt;/strong>：在视觉质量、图像细节和输出多样性等方面达到了新高度，生成的图像更加逼真、细腻，细节丰富度高，风格多样，能与闭源的 Midjourney v6.1 模型不相上下，甚至在一些方面表现更优，如复杂场景生成、人物细节处理等。&lt;/li>
&lt;li>&lt;strong>提示词遵循能力&lt;/strong>：对提示词的理解和遵循能力很强，能够准确地根据输入的文本描述生成符合要求的图像，包括对复杂指令、长文本描述以及特定场景和细节要求的准确呈现，减少了因提示词理解不准确而导致的图像与预期不符的问题。&lt;/li>
&lt;li>&lt;strong>文字生成能力&lt;/strong>：在图像中生成文字的效果出色，可以处理重复字母等棘手情况，生成的文字内容准确、排版合理，这在一些需要在图像中呈现文字信息的场景中具有很大优势，如生成带有文字标识的产品图片、包含文字说明的场景图片等。&lt;/li>
&lt;li>&lt;strong>手部细节处理&lt;/strong>：相较于之前的一些模型，Flux 在手部细节生成上有了显著改进，减少了手指等部位的畸形或错误，使生成的人物图像更加自然、真实。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>不同版本特点&lt;/strong>：&lt;br>
有 3 个核心版本，Pro/dev/schnell
&lt;ul>
&lt;li>&lt;strong>FLUX (pro)&lt;/strong>：闭源模型，自家压箱底的货。具备最佳性能，包括最先进的提示跟随能力、视觉质量、图像细节和输出多样性，适用于专业用户和对图像质量要求极高的场景，可通过注册官方 API 申请访问权限，同时支持企业定制。&lt;/li>
&lt;li>&lt;strong>FLUX (dev)&lt;/strong>：开源模型，不可商用，直接从 FLUX (pro) 蒸馏而来，具备相似的图像质量和提示词遵循能力，但更高效，适合开发者进行研究和实验等非商业用途的使用。&lt;/li>
&lt;li>&lt;strong>FLUX (schnell)&lt;/strong>：开源模型，可&lt;!-- raw HTML omitted -->商用&lt;!-- raw HTML omitted -->，专门为本地开发和个人使用量身定制，生成速度最快，内存占用最小，是个人用户体验和进行简单开发测试的不错选择。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>初识 Diffusion（概念）</title><link>https://www.jsongo.top/articles/diffusion-101/</link><pubDate>Sun, 24 Nov 2024 11:26:10 +0800</pubDate><guid>https://www.jsongo.top/articles/diffusion-101/</guid><description>&lt;img src="https://cdn.ethango.top/banners/96539755245eb88510f037ed2d24f60d.jpeg" alt="Featured image of post 初识 Diffusion（概念）" />&lt;p>Diffusion（扩散模型）是一类生成式模型，在机器学习和人工智能领域应用广泛，特别是在图像生成、语音合成等领域发挥了重要作用。&lt;/p>
&lt;h1 id="model--checkpoint">Model &amp;amp; Checkpoint
&lt;/h1>&lt;p>开源模型发布出来的文件一般都很大，里面包含了很多模型参数、快照等。Model 和 checkpoint 在开源这个层面上，两个概念可以认为大体上是等价的，只不过我们一般不叫 checkpoint，而是说某个 model 开源了。&lt;/p>
&lt;ul>
&lt;li>Model 是一个比较抽象的概念，它包括模型的架构（例如神经网络中的层数、每层神经元数量、连接方式等）和训练目标（例如是进行分类任务、生成任务还是回归任务）&lt;/li>
&lt;li>Checkpoint 是在模型训练过程中定期保存的模型参数快照，与 Model 密切相关。Checkpoint 包含了 Model 的架构定义、参数值、优化器状态以及训练过程的元数据等，是对 Model 在特定训练阶段状态的完整记录。&lt;/li>
&lt;/ul>
&lt;h1 id="clip">CLIP
&lt;/h1>&lt;h2 id="概念">概念
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>CLIP 模型&lt;/strong>：CLIP 是 Contrastive Language-Image Pre-training 的缩写，由 OpenAI 在 2021 年 1 月 5 日发布，是一种将计算机视觉与自然语言处理相结合的神经网络模型。
&lt;ul>
&lt;li>它通过对 400,000,000 组（图像，文本）对数据进行预训练，从而能够在给定图像的情况下，根据自然语言指令预测出最相关的文本片段，展现出了类似 GPT-2 和 GPT-3 的 zero-shot 学习能力，即模型可以在未针对特定任务进行直接优化训练的情况下，对未曾见过的数据类别进行较好地预测&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>CLIP Vision&lt;/strong>：CLIP Vision 主要负责处理视觉信息，也就是对输入的图像数据进行特征提取和编码等操作，将图像转化为模型能够理解和处理的向量表示，以便与文本信息进行对比学习和关联。&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>
&lt;ul>
&lt;li>例如，在图像生成领域，CLIP Vision 可以帮助模型理解图像的内容和特征，从而生成更符合语义描述的图像。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>CLIP Text&lt;/strong>：我们常看到的这个 CLIP Text 概念，主要是指作为参考文本来对图像进行调整的文本描述。&lt;/li>
&lt;/ul>
&lt;h2 id="应用">应用
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>图像生成&lt;/strong>：如在 Paints-Undo 项目中，CLIP Vision 作为模型架构的一部分，与其他组件共同作用，通过对输入图像的处理和分析，为生成模拟人类绘画过程的动画提供视觉信息基础，帮助模型理解图像的内容和结构，从而更准确地生成绘画过程中的各个中间状态。&lt;/li>
&lt;li>&lt;strong>图像分类与标注&lt;/strong>：可以根据图像的视觉特征，结合预训练时学习到的图像与文本的关联，对未见过的图像进行分类或自动生成相应的文本标注，例如判断一张图片是风景照、人物照还是动物照，并给出相应的文字描述。&lt;/li>
&lt;li>&lt;strong>图像检索&lt;/strong>：基于 CLIP Vision 对图像特征的提取和与文本的关联能力，可以实现根据文本描述来检索相关的图像，或者根据图像来查找与之语义相关的文本信息，提高图像检索的准确性和效率。&lt;/li>
&lt;li>&lt;strong>视觉问答系统&lt;/strong>：帮助系统理解图像中的视觉内容，结合对自然语言问题的理解，生成准确的文本答案，例如回答关于图像中物体的位置、颜色、数量等问题 。&lt;/li>
&lt;/ul>
&lt;h1 id="vae">VAE
&lt;/h1>&lt;p>VAE 主要用于将图像数据压缩到一个潜在空间，然后再从这个潜在空间中生成新的图像，侧重于图像的生成和重建。&lt;/p>
&lt;h2 id="latent-image">Latent Image
&lt;/h2>&lt;p>经过某种变换或编码后隐藏在数据中的图像信息。例如，在使用变分自编码器（VAE）进行图像生成或处理时，图像数据会被压缩到一个潜在空间（latent space），这个潜在空间中的向量可以被看作是潜像的一种表示形式。这些潜像向量包含了图像的关键特征，如形状、颜色、纹理等信息，通过解码器可以将这些潜像向量转换回可见的图像。&lt;/p>
&lt;h1 id="lora">Lora
&lt;/h1>&lt;p>LoRA 是一种用于微调预训练模型的技术，通过在原始模型的基础上添加少量可训练的参数来实现对模型的微调。&lt;/p>
&lt;hr>
&lt;h1 id="footnotes">Footnotes
&lt;/h1>&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>简单说就是，主要用于处理和理解图像信息，能够提取图像的特征表示，进而与文本特征进行对比和匹配等操作，以实现如根据文本描述生成相应图像、图像分类、图像检索等多种与图像和文本相关的任务&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>