<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI/工具 on Ethan 的思考札记</title><link>https://www.jsongo.top/tags/ai/%E5%B7%A5%E5%85%B7/</link><description>Recent content in AI/工具 on Ethan 的思考札记</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Tue, 19 Nov 2024 16:36:27 +0800</lastBuildDate><atom:link href="https://www.jsongo.top/tags/ai/%E5%B7%A5%E5%85%B7/index.xml" rel="self" type="application/rss+xml"/><item><title>Stable Diffusion 生图技术（一）</title><link>https://www.jsongo.top/articles/stable-diffusion-gen-image-1/</link><pubDate>Tue, 19 Nov 2024 16:36:27 +0800</pubDate><guid>https://www.jsongo.top/articles/stable-diffusion-gen-image-1/</guid><description>&lt;img src="https://cdn.jsongo.top/banners/772c64fbb07e3cd46573602f922a7829.jpg" alt="Featured image of post Stable Diffusion 生图技术（一）" />&lt;h1 id="概述">概述
&lt;/h1>&lt;h2 id="背景">背景
&lt;/h2>&lt;p>Stable Diffusion 最初是由德国慕尼黑大学的 CompVis 研究小组、纽约的 RunwayML 公司等组成的国际研究团队开发的，后来 Stability AI 参与其中并推动了其发展。&lt;br>
Stable Diffusion 的发展历程如下：&lt;br>
&lt;img src="https://cdn.jsongo.top/2024/12/afb65322de6b531c0c01e86e6d27c0e0.webp"
loading="lazy"
alt="|900"
>&lt;/p>
&lt;h2 id="技术简介">技术简介
&lt;/h2>&lt;p>Stable Diffusion 在模型架构中采用了 Transformer 架构的一些特性，基于扩散模型架构来生成图片。&lt;br>
它的原理用白话说，比较简单：加噪和去噪（专业术语叫前向扩散和反向扩散）。加噪后的图片主要用于训练或作为初始输入（latent），然后训练一个模型去实现某些图像生成目标（去噪过程）。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>前向扩散&lt;/strong>：在前向阶段，通过向原始图像数据（如真实的照片或绘画）添加噪声，逐步将图像转换为纯噪声。这个过程是在多个时间步（time - steps）中完成的，每一步都按照一定的规则（通常是基于高斯分布）添加噪声，使得图像信息逐渐被噪声掩盖。例如，开始时图像可能还比较清晰，随着时间步的增加，图像越来越模糊，最终变成完全的噪声。&lt;/li>
&lt;li>&lt;strong>反向扩散&lt;/strong>：这是生成图像的关键阶段。从纯噪声开始，模型通过学习到的去噪过程，逐步恢复图像信息。模型会预测每个时间步中需要去除的噪声，经过多个时间步的迭代，最终生成一张类似于训练数据分布的图像。这个过程类似于从无序的噪声中逐渐 “雕刻” 出有意义的图像。&lt;/li>
&lt;/ul>
&lt;h1 id="生图过程">生图过程
&lt;/h1>&lt;p>&lt;img src="https://cdn.jsongo.top/2024/11/d66a5473be54d712b3aa8e879f9a8c3a.webp"
loading="lazy"
alt="|700"
>&lt;/p>
&lt;h2 id="模型-checkpoint跟采样的关系">模型 （Checkpoint）跟采样的关系
&lt;/h2>&lt;p>Checkpoint 包含了模型在特定训练阶段的所有权重、偏置以及优化器的状态等信息，而采样模型的参数是由 Checkpoint 所确定的，采样模型使用该 Checkpoint 中存储的权重和其他参数进行计算。不同的 Checkpoint 会导致采样模型在生成图像时表现出不同的性能和风格。例如，某些 Checkpoint 可能侧重于生成高分辨率的图像，而另一些可能更擅长生成具有特定艺术风格的图像。这是因为在训练过程中，不同的 Checkpoint 所对应的训练数据和训练目标可能有所不同，从而影响了采样模型的行为。&lt;/p>
&lt;h2 id="clip-text-encode">CLIP Text Encode
&lt;/h2>&lt;p>属于 Condition 节点，它用 CLIP 模型对文本 Prompt 进行编码，对模型生成的结果进行方向上的引导，其实可能理解为文本模型中的 embedding。&lt;/p>
&lt;h2 id="采样">采样
&lt;/h2>&lt;p>生图其实就是一个反向扩散的过程，从一个完全是噪声的图像开始，通过模型逐步去除噪声来生成图像。这个过程通过一个采样函数来实现，它基于模型预测的噪声来更新噪声图像。&lt;br>
&lt;img src="https://cdn.jsongo.top/2024/12/dd2433499e7a9229aa5bd816177c8988.webp"
loading="lazy"
>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Steps&lt;/strong>，迭代生图，每次更新噪声图像，需要经过多个 Step 的迭代。随着 Step 反复执行，图像中的噪声逐渐减少，最终生成一个近似原始图像分布的图像。不过生成的图像质量可能会受到多种因素的影响，如模型的性能、采样方法、时间步数等。在实际应用中，可能需要对生成的图像进行后处理（如调整颜色、对比度等）来提高图像质量。&lt;/li>
&lt;li>&lt;strong>seed&lt;/strong>，种子值，用于初始化随机数生成器。相同的种子值在相同的模型和参数设置下会生成相同的图像，这有助于复现结果。&lt;/li>
&lt;li>&lt;strong>control_after_generate&lt;/strong>：这个参数可能与生成后的控制操作有关，例如对生成的图像进行随机化处理&lt;/li>
&lt;li>&lt;strong>steps&lt;/strong>：指生成过程中的迭代步数。步数越多，生成的图像通常会越精细，但也会增加计算时间。&lt;/li>
&lt;li>&lt;strong>cfg&lt;/strong>（Classifier-Free Guidance）：无分类器引导的强度，它是一种在生成式模型（如 Stable Diffusion）中用于引导图像生成方向的技术，决定了模型在生成图像时对正向提示（你希望在图像中出现的内容）的遵循程度。较高的 CFG 值会使生成的图像更紧密地遵循正向 Prompt，但也有可能导致图像过度拟合。
&lt;ul>
&lt;li>取值范围：
&lt;ul>
&lt;li>CFG 的值通常在 1 到 20 之间，理论上可以取更高的值，但在实际应用中，过高的值往往会导致图像质量下降。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>较低值（1 - 4）：
&lt;ul>
&lt;li>当 CFG 值较低时，生成的图像会更具随机性和创造性。模型对正向提示的遵循程度较弱，因此可能会生成一些与提示不太相关但具有独特创意的图像。这种情况下，图像可能会包含一些意外的元素或风格。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>中等值（4 - 10）：
&lt;ul>
&lt;li>在这个范围内，是比较常用的取值。模型会在遵循正向提示和保持一定的创造性之间取得较好的平衡。能够生成与提示较为符合的图像，同时也不会显得过于刻板。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>较高值（10 - 20）：
&lt;ul>
&lt;li>当 CFG 值较高时，生成的图像会非常紧密地遵循正向提示。这可能会导致图像过于 “完美” 地符合提示，但也可能会出现一些问题，比如图像的细节可能会显得不自然，色彩可能会过于饱和，或者图像可能会失去一些自然的随机性和美感&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>sampler_name&lt;/strong>：这是指采样器的名称。不同的采样器（如 Euler、Langevin 等）有不同的特性，会影响图像生成的速度和质量。&lt;/li>
&lt;li>&lt;strong>scheduler&lt;/strong>：调度器，用于控制生成过程中的步长和其他参数。不同的调度器会影响生成的效率和结果。&lt;/li>
&lt;li>&lt;strong>denoise&lt;/strong>：去噪参数控制生成过程中对噪声的去除程度。较低的去噪值会使生成的图像更具随机性，而较高的值会使图像更平滑和可预测。&lt;/li>
&lt;/ul>
&lt;h2 id="vae-decoder变分自解码器">VAE Decoder（变分自解码器）
&lt;/h2>&lt;p>图像数据往往是高维的，包含大量的像素信息，所以一般训练和计算时，往往要压缩到低维来处理，用更紧凑的方式表示图像的关键特征，就像文本模型中的 Embedding 处理，变成稠密的数据，在图像处理领域叫潜在空间（Latent Space）。而解码器则是将潜在空间中的表示再转换回图像数据空间，尽可能地重建原始图像。&lt;br>
生成新图的过程，由于潜在空间是连续的，稍微改变潜在空间中的向量值，就可以生成与原始图像在某些特征上有所变化的新图像。例如，在生成人脸图像的 VAE 模型中，在潜在空间中沿着某个方向移动向量可能会改变人脸的表情、发型或者年龄等特征。&lt;/p>
&lt;h2 id="latent-image">Latent Image
&lt;/h2>&lt;p>一般在整个 ComfyUI 流程中，会插入一张空的（Empty） Lantent Image，它提供了一个初始的 “画布”，让模型在这个基础上进行生成或转换操作。&lt;br>
当然如果是对已有的图片进行调整、修复或优化，则可以把空的图换成一张现成的图片，这时就要把它加载并转换为合适的潜在表示形式后，可以作为生成过程的起点。在 ComfyUI 中，可能涉及到将图片通过适当的预处理步骤，如调整大小、归一化等操作后，然后将其编码为潜在空间中的表示，这样就可以基于已有的图像内容进行修改、风格转换或者其他生成操作。&lt;/p>
&lt;h1 id="stability-ai-官方">Stability AI 官方
&lt;/h1>&lt;p>Stable Diffusion 的官方可用的模型，可以从 API 文档中看到： &lt;a class="link" href="https://platform.stability.ai/pricing" target="_blank" rel="noopener"
>Stability AI - Developer Platform&lt;/a>。最新的是 SD 3.5（2024 年 11 月）。&lt;/p>
&lt;ul>
&lt;li>1 credit = $0.01&lt;/li>
&lt;li>这么算，生成一张 SD 3.5 的图片，medium 要 $0.035，差不多 0.25 元，4 张 1 块。&lt;br>
&lt;img src="https://cdn.jsongo.top/2024/11/207f1abc8b1be591d3f76ec236344c3c.webp"
loading="lazy"
alt="736deaffc1c02497df089d14539fba5d_MD5|700"
>&lt;/li>
&lt;/ul>
&lt;h2 id="体验">体验
&lt;/h2>&lt;p>&lt;a class="link" href="https://stabledifffusion.com/tools/ai-image-generator" target="_blank" rel="noopener"
>Free AI Image Generator - Turn Text to Image Online | Stable Diffusion Online&lt;/a>&lt;/p>
&lt;h1 id="字节的-lighting-模型">字节的 Lighting 模型
&lt;/h1>&lt;p>&lt;a class="link" href="https://huggingface.co/ByteDance/SDXL-Lightning" target="_blank" rel="noopener"
>ByteDance/SDXL-Lightning at main&lt;/a> 字节的这个模型生成效果相当不错。&lt;br>
它同时提供了 Full UNet 和 LoRA 版本，都是相对比较小的蒸馏模型（虽然 UNet 也有几个 G）。&lt;/p>
&lt;blockquote>
&lt;p>We provide both full UNet and LoRA checkpoints. The full UNet models have the best quality while the LoRA models can be applied to other base models.&lt;/p>
&lt;/blockquote>
&lt;h2 id="小科普可跳过">小科普（可跳过）
&lt;/h2>&lt;h3 id="概念">概念
&lt;/h3>&lt;ul>
&lt;li>UNet 是卷积神经网络（CNN）的一种特殊架构，在生成对抗网络 (GANs) 和扩散模型中广泛使用。它是一种完整的网络架构，专门用于图像分割任务，从输入图像到输出分割结果，有自己独立的结构和训练流程。&lt;/li>
&lt;li>LoRA 不是一种独立的网络架构，而是一种模型微调策略，可以应用于各种现有的预训练模型（包括但不限于基于 UNet 架构的模型），用于在不大量修改原始模型结构的情况下进行任务适配。&lt;/li>
&lt;/ul>
&lt;h3 id="从参数规模与训练成本来看">从参数规模与训练成本来看
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>UNet&lt;/strong>：在训练过程中，需要对整个网络的参数进行学习和更新，尤其是在处理高分辨率图像或复杂任务时，可能需要大量的训练数据和计算资源。&lt;/li>
&lt;li>&lt;strong>LoRA&lt;/strong>：通过低秩分解减少了需要训练的参数数量，在对大型预训练模型进行微调时，训练成本显著降低，对数据量的要求也相对较少。&lt;br>
UNet 的模型一般都比较大：&lt;br>
&lt;img src="https://cdn.jsongo.top/2024/11/8f1f152a35835044e0ea8fc381a9333b.webp"
loading="lazy"
alt="|475"
>&lt;br>
LoRA 则小很多&lt;br>
&lt;img src="https://cdn.jsongo.top/2024/11/36b48790266f80b2cb03cfc22ded0f13.webp"
loading="lazy"
alt="|475"
>&lt;/li>
&lt;/ul>
&lt;h2 id="使用">使用
&lt;/h2>&lt;p>它们都有 2-Step, 4-Step, 8-Step，其中 1-step 只是实验性的、效果不好、质量不稳定，一般建议用折中的 4-step，如果资源充足可以选质量最好的 8-step。&lt;br>
ComfyUI 中的使用非常简单：&lt;br>
&lt;img src="https://cdn.jsongo.top/2024/11/fc799454d0483ec8b7fea253e7ce45e4.webp"
loading="lazy"
alt="|675"
>&lt;br>
如果只是想玩玩，可以直接在 huggingface 上试试：&lt;a class="link" href="https://huggingface.co/spaces/ByteDance/SDXL-Lightning" target="_blank" rel="noopener"
>SDXL-Lightning spaces&lt;/a> ，效果还是不错的：&lt;br>
&lt;img src="https://cdn.jsongo.top/2024/11/22638b08323398c180c2d2ff0d1e59f8.webp"
loading="lazy"
alt="|700"
>&lt;/p></description></item></channel></rss>