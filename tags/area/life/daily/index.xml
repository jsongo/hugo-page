<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Area/Life/Daily on Ethan 的思考札记</title><link>https://www.jsongo.top/tags/area/life/daily/</link><description>Recent content in Area/Life/Daily on Ethan 的思考札记</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><atom:link href="https://www.jsongo.top/tags/area/life/daily/index.xml" rel="self" type="application/rss+xml"/><item><title>图像生成：角色场景形象适配</title><link>https://www.jsongo.top/articles/ipadapter-lora-controlnet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.jsongo.top/articles/ipadapter-lora-controlnet/</guid><description>&lt;img src="https://cdn.jsongo.top/banners/3c463375cc35ddc5e78d71b95ddcbd51.jpeg" alt="Featured image of post 图像生成：角色场景形象适配" />&lt;hr>
&lt;h1 id="几种方案">几种方案
&lt;/h1>&lt;h2 id="ip-adapter">IP-Adapter
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>原理&lt;/strong>：IP-Adapter 的核心是其 “解耦交叉注意力机制”，它将文本特征和图像特征的处理分开，为每种输入类型创建了不同的路径，从而能够更好地将视觉提示信息整合到模型中，同时不影响模型对文本指令的理解和执行能力。通过这种机制，IP-Adapter 可以识别参考图的艺术风格、内容以及特定的角色形象等信息，并生成与之相似的作品.&lt;/li>
&lt;li>&lt;strong>使用方式&lt;/strong>：以 Stable Diffusion 为例，需先下载对应版本的 IP-Adapter 预处理器和模型，如 sd1.5 版本的 “ip-adapter_clip_sd15” 预处理器及相关模型 。在使用时，将打斗场景图作为输入图像，将角色形象图作为参考图，通过调整相关参数和权重，IP-Adapter 就能根据角色形象图的特征对打斗场景图中的角色进行适配和修改，生成符合预期的新图像.&lt;/li>
&lt;/ul>
&lt;h2 id="dreambooth">DreamBooth
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>原理&lt;/strong>：DreamBooth 是一种基于扩散模型的微调技术，它允许用户使用少量的特定主题图像（如你的角色形象图）对预训练的文本到图像生成模型进行微调，使模型能够学习到与该主题相关的特定特征和细节，从而在生成图像时能够更准确地生成符合该主题的内容。&lt;/li>
&lt;li>&lt;strong>使用方式&lt;/strong>：首先收集包含角色形象的高质量图像数据集，然后选择一个预训练的基础模型，如 Stable Diffusion 等。使用 DreamBooth 技术对基础模型进行微调，将角色形象与特定的唯一标识符绑定，让模型学习到角色的特征。最后，将微调后的模型应用于打斗场景图，通过输入相关的文本提示，模型会根据学习到的角色特征对场景图中的角色进行适配和生成。&lt;/li>
&lt;/ul>
&lt;h2 id="lora-low-rank-adaptation-of-large-language-models">LoRA (Low-Rank Adaptation of Large Language Models)
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>原理&lt;/strong>：LoRA 的基本思想是通过在大型语言模型的基础上，添加少量的可训练参数来实现对模型的微调，从而在保持模型原有性能的基础上，快速适应特定的任务或领域。对于图像生成任务，同样可以利用类似的原理，通过对预训练的图像生成模型添加 LoRA 模块，使其能够根据特定的角色形象图进行微调，学习到角色的特征表示，进而在生成图像时能够更好地适配这些角色形象。&lt;/li>
&lt;li>&lt;strong>使用方式&lt;/strong>：先确定预训练的图像生成模型，然后针对你的角色形象图准备相应的数据集。接着，设计并训练适合该模型的 LoRA 模块，使其能够捕捉到角色形象的关键特征。在实际应用中，将训练好的 LoRA 模块与预训练模型结合，输入包含打斗场景和角色相关的文本提示，模型就会根据 LoRA 模块中学习到的角色特征，对场景图中的角色进行相应的适配和生成。&lt;/li>
&lt;/ul>
&lt;h2 id="controlnet">ControlNet
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>原理&lt;/strong>：ControlNet 是一种用于控制图像生成过程的神经网络架构，它可以通过添加额外的条件控制信息来引导图像生成模型生成符合特定要求的图像。在你的需求中，可以将角色形象图作为 ControlNet 的输入条件，通过网络中的特定模块对生成过程进行约束和引导，使生成的打斗场景图中的角色能够更好地适配给定的角色形象.&lt;/li>
&lt;li>&lt;strong>使用方式&lt;/strong>：选择合适的 ControlNet 预训练模型，并将其与基础的图像生成模型结合，如 Stable Diffusion。将打斗场景图作为基础图像输入，同时将角色形象图作为 ControlNet 的条件输入，通过调整 ControlNet 中的各种参数和权重，如控制线条、色彩、形状等方面的参数，来精确控制生成的图像中角色的外观和姿态，使其与给定的角色形象相匹配.&lt;/li>
&lt;/ul>
&lt;h1 id="成本分析">成本分析
&lt;/h1>&lt;ul>
&lt;li>&lt;strong>IP-Adapter&lt;/strong>：IP-Adapter 无需进行模型训练，主要成本在于寻找符合预期的参考图，其成本相对较低，能在几分钟内看到结果，大大节省了时间.&lt;/li>
&lt;li>&lt;strong>DreamBooth&lt;/strong>：训练成本相对较高，需要至少 20 张不同角度的人物图像，并且对硬件要求较高，至少需要 24GB 的显存以及性能较好的 GPU，若使用云服务训练则还需额外产生费用 2.&lt;/li>
&lt;li>&lt;strong>LoRA&lt;/strong>：LoRA 可以冻结预训练模型的全部参数权重，然后只在新增的网络层去做训练，训练数据量可根据具体需求而定，因此成本相对较低.&lt;/li>
&lt;li>&lt;strong>ControlNet&lt;/strong>：ControlNet 的训练成本相对较高。例如，其 canny edge detector 模型的训练需要 300 万张边缘 - 图像 - 标注对的语料，在 A100 80g 的环境下要花费 600 个 gpu 小时；human pose 模型则需 8 万张姿态 - 图像 - 标注对的语料及 400 个 gpu 时.&lt;/li>
&lt;/ul>
&lt;h1 id="思路">思路
&lt;/h1>&lt;h2 id="lora">LoRA
&lt;/h2>&lt;h3 id="过拟合问题">过拟合问题
&lt;/h3>&lt;p>需要准备多张形象比较一致的图片，要不然只有一张图会有&lt;strong>过拟合问题&lt;/strong>，以及&lt;strong>特征不全面&lt;/strong>的问题。简单说，仅靠一张图片，LoRA 可能无法学习到角色的完整特征，导致在生成场景图中角色的其他角度或状态时出现不匹配的情况。例如，角色在训练图片中是正面微笑的形象，在生成侧面打斗场景时，可能会出现面部结构变形或者表情不自然的问题。&lt;br>
这个方法可以作为后续优化的方向，当可以生成金乡形象一致的图片时，再来走这个思路。&lt;/p>
&lt;h3 id="猫箱类软件">猫箱类软件
&lt;/h3>&lt;p>这些软件会让使用者先上传几张自己的图片，之后它其实会拿这些去做 LoRA 训练（也可能是其它方式的微调），之后再用这个用户专属的模型去生成他后续的各场景图。&lt;/p>
&lt;h2 id="controlnet-1">ControlNet
&lt;/h2>&lt;h3 id="硬件要求">硬件要求
&lt;/h3>&lt;p>ControlNet 需要一定的硬件支持来运行，由于它在原始扩散模型基础上增加了控制模块，对 GPU 等硬件资源的需求相对较高，特别是在处理复杂的控制条件和高分辨率图像生成时，可能需要更强大的 GPU 来保证生成速度和质量.&lt;/p>
&lt;h3 id="时间成本">时间成本
&lt;/h3>&lt;p>训练 ControlNet 所需的时间较长，从数据收集、标注到模型训练完成，整个过程可能需要数天甚至数周，这取决于训练数据的规模和硬件设备的性能.&lt;/p>
&lt;blockquote>
&lt;p>总体看，成本上不一定划得来。相比之下，IP-Adapter 不需要训练模型，只需找到符合预期的参考图即可开始生成，大大节省了时间和资源，能够在几分钟内看到结果，其模型参数只有 22m，存储占用小，对硬件资源的要求相对较低，下面展开讲下&lt;/p>
&lt;/blockquote>
&lt;h2 id="ip-adapter-1">IP-Adapter
&lt;/h2>&lt;p>IP-Adapter 确实有其独特的优势，使其成为一种很有吸引力的图像生成辅助方法，以下是对其优点的详细介绍：&lt;/p>
&lt;h3 id="无需训练模型">无需训练模型
&lt;/h3>&lt;p>与需要收集大量数据并进行长时间训练的 LoRA 等方法不同，IP-Adapter 不需要训练模型，大大节省了时间和资源，仅需找到符合预期的参考图即可开始生成，能在几分钟内看到结果，效率极高.&lt;/p>
&lt;h3 id="单图即可实现风格迁移">单图即可实现风格迁移
&lt;/h3>&lt;p>只需一张图片就能实现风格迁移，将该图片的风格特征融入到生成的图像中。例如，如果有一张具有独特画风的角色图片，IP-Adapter 可以把这种画风应用到新生成的打斗场景图中，让场景图中的角色呈现出与参考图一致的风格，为生成特定风格的图像提供了极大的便利 1.&lt;/p>
&lt;h3 id="支持多图多特征提取">支持多图多特征提取
&lt;/h3>&lt;p>可以同时读取多张参考图，并提取它们的特征，从而使生成的结果拥有更丰富的多样性和随机性。比如，你有几张同一角色在不同状态下的图片，IP-Adapter 能够综合这些图片的特征，生成更全面、更具变化的角色形象，使其在打斗场景中的表现更加自然和丰富.&lt;/p>
&lt;h3 id="与-prompt-配合度高">与 Prompt 配合度高
&lt;/h3>&lt;p>对 Prompt 有强注意力，能使 Prompt 中的信息更直观地反映在生成结果中。用户可以通过替换 Prompt 里的关键词，在继承参考图风格的同时，指向不同的结果，形成 Prompt 的组合矩阵，进一步拓展生成结果的多样性。例如，在生成打斗场景图时，通过修改 Prompt 中的关键词，如 “激烈打斗”“轻松打斗” 等，可以生成不同氛围和情节的场景图，且都带有参考图的风格特征.&lt;/p>
&lt;h3 id="兼容性强">兼容性强
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>适配多种基础模型&lt;/strong>：IP-Adapter 基于预训练的文本到图像扩散模型开发，具有很强的通用性和兼容性，可以适配 Stable Diffusion 等多种基础模型，而无需针对不同的基础模型进行重新训练或调整.&lt;/li>
&lt;li>&lt;strong>可与 ControlNet 等结合&lt;/strong>：能够与 ControlNet 等其他可控生成工具结合使用，实现更强大的图像生成控制能力。比如，通过 ControlNet 可以对生成图像的线条、边缘等细节进行控制，再结合 IP-Adapter 的风格迁移和内容引导，能够生成更加精细、准确且符合风格要求的图像 1.&lt;/li>
&lt;/ul>
&lt;h3 id="模型参数小">模型参数小
&lt;/h3>&lt;p>IP-Adapter 本身的模型参数只有 22m，存储占用小，对硬件资源的要求相对较低，在普通的硬件设备上也能够较为流畅地运行，降低了使用门槛，更便于广泛应用和推广.&lt;/p>
&lt;h1 id="方案选择">方案选择
&lt;/h1>&lt;p>最终还是决定先用 IP-Adapter 来推进。&lt;/p></description></item></channel></rss>